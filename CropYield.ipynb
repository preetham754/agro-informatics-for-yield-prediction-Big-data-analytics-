{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e93c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark packages\n",
    "from pyspark import sql, SparkConf, SparkContext\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4477a322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/29 15:43:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2416)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2416)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:329)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#initialize spark session\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m spark_session = \u001b[43msql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHDFS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m spark_context = SparkContext.getOrCreate(SparkConf().setAppName(\u001b[33m\"\u001b[39m\u001b[33mHDFS\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      4\u001b[39m logs = spark_context.setLogLevel(\u001b[33m\"\u001b[39m\u001b[33mERROR\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pyspark/sql/session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pyspark/context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pyspark/context.py:203\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    201\u001b[39m SparkContext._ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway=gateway, conf=conf)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28mself\u001b[39m.stop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pyspark/context.py:296\u001b[39m, in \u001b[36mSparkContext._do_init\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m.environment[\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m] = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[38;5;28mself\u001b[39m._jsc = jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;28mself\u001b[39m._conf = SparkConf(_jconf=\u001b[38;5;28mself\u001b[39m._jsc.sc().conf())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pyspark/context.py:421\u001b[39m, in \u001b[36mSparkContext._initialize_context\u001b[39m\u001b[34m(self, jconf)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[33;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/py4j/java_gateway.py:1587\u001b[39m, in \u001b[36mJavaClass.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1581\u001b[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001b[32m   1582\u001b[39m     \u001b[38;5;28mself\u001b[39m._command_header +\\\n\u001b[32m   1583\u001b[39m     args_command +\\\n\u001b[32m   1584\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1586\u001b[39m answer = \u001b[38;5;28mself\u001b[39m._gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1587\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1591\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2416)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2416)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:329)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\n"
     ]
    }
   ],
   "source": [
    "#initialize spark session\n",
    "spark_session = sql.SparkSession.builder.appName(\"HDFS\").getOrCreate()\n",
    "spark_context = SparkContext.getOrCreate(SparkConf().setAppName(\"HDFS\"))\n",
    "logs = spark_context.setLogLevel(\"ERROR\")\n",
    "print(\"Spark session initialize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection parameters for spark to Amazon S3\n",
    "spark_session._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", \"AKIAUGQ637RYKQTW47FH\")\n",
    "spark_session._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", \"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\")\n",
    "spark_session._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "spark_session._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "spark_session._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider\")\n",
    "spark_session._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.eu-west-1.amazonaws.com\")\n",
    "print(\"Connection to S3 Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d8d0a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark_session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#code to load and display dataset from S3 using spark session object\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dataset = \u001b[43mspark_session\u001b[49m.read.csv(\u001b[33m'\u001b[39m\u001b[33ms3n://cropyielddata/yield_df.csv\u001b[39m\u001b[33m'\u001b[39m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m, header=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m dataset.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'spark_session' is not defined"
     ]
    }
   ],
   "source": [
    "#code to load and display dataset from S3 using spark session object\n",
    "dataset = spark_session.read.csv('s3n://cropyielddata/yield_df.csv', inferSchema=True, header=True)\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eeb42e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#describing dataset with details like count, mean, standard deviation of each dataset attributes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdataset\u001b[49m.toPandas().describe()\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#describing dataset with details like count, mean, standard deviation of each dataset attributes\n",
    "dataset.toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6803656",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#visualizing distribution of numerical data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdataset\u001b[49m.toPandas().hist(figsize=(\u001b[32m10\u001b[39m, \u001b[32m8\u001b[39m))\n\u001b[32m      3\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mHistogram distribution of dataset values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m plt.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#visualizing distribution of numerical data\n",
    "dataset.toPandas().hist(figsize=(10, 8))\n",
    "plt.title(\"Histogram distribution of dataset values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6433b6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#graph of different countries found in dataset for making crop yield\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functions\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m areas = \u001b[43mdataset\u001b[49m.select(\u001b[33m'\u001b[39m\u001b[33mArea\u001b[39m\u001b[33m'\u001b[39m).filter(functions.col(\u001b[33m'\u001b[39m\u001b[33mArea\u001b[39m\u001b[33m'\u001b[39m).isNotNull()).toPandas().values.ravel()\n\u001b[32m      4\u001b[39m names, count = np.unique(areas, return_counts = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m height = count\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#graph of different countries found in dataset for making crop yield\n",
    "from pyspark.sql import functions\n",
    "areas = dataset.select('Area').filter(functions.col('Area').isNotNull()).toPandas().values.ravel()\n",
    "names, count = np.unique(areas, return_counts = True)\n",
    "height = count\n",
    "bars = names\n",
    "y_pos = np.arange(len(bars))\n",
    "plt.figure(figsize = (14, 3)) \n",
    "plt.bar(y_pos, height)\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.xlabel(\"Different Area Graph for Crop Yield\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a207264",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark_session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#visualizing graph of different crops found in dataset\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SQLContext\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m sql = SQLContext(\u001b[43mspark_session\u001b[49m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#converting spark dataframe to spark sql query\u001b[39;00m\n\u001b[32m      5\u001b[39m dataset.registerTempTable(\u001b[33m\"\u001b[39m\u001b[33mcrop\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'spark_session' is not defined"
     ]
    }
   ],
   "source": [
    "#visualizing graph of different crops found in dataset\n",
    "from pyspark.sql import SQLContext\n",
    "sql = SQLContext(spark_session)\n",
    "#converting spark dataframe to spark sql query\n",
    "dataset.registerTempTable(\"crop\")\n",
    "df = sql.sql(\"SELECT Item from crop\")\n",
    "df = df.toPandas()\n",
    "unique, count = np.unique(df['Item'], return_counts=True)\n",
    "values = []\n",
    "for i in range(len(unique)):\n",
    "    values.append([unique[i], count[i]])\n",
    "values = pd.DataFrame(values, columns = ['Crop', 'Count'])   \n",
    "plt.figure(figsize=(8,3))\n",
    "sns.barplot(x='Crop',y='Count', data=values)\n",
    "plt.title('Most Common Crop Yield by Different Countries')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfe82e09",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql' has no attribute 'sql'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#query to visualize different yield of crop by different countries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43msql\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mSELECT Area, Item, yield from crop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m df = df.toPandas()\n\u001b[32m      4\u001b[39m data = df.groupby([\u001b[33m'\u001b[39m\u001b[33mItem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mArea\u001b[39m\u001b[33m'\u001b[39m])[\u001b[33m'\u001b[39m\u001b[33myield\u001b[39m\u001b[33m'\u001b[39m].sum().sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).nlargest(\u001b[32m30\u001b[39m).reset_index()\n",
      "\u001b[31mAttributeError\u001b[39m: module 'pyspark.sql' has no attribute 'sql'"
     ]
    }
   ],
   "source": [
    "#query to visualize different yield of crop by different countries\n",
    "df = sql.sql(\"SELECT Area, Item, yield from crop\")\n",
    "df = df.toPandas()\n",
    "data = df.groupby(['Item', 'Area'])['yield'].sum().sort_values(ascending=False).nlargest(30).reset_index()\n",
    "sns.catplot(x=\"Item\", y=\"yield\", hue='Area', data=data, kind='point')\n",
    "plt.title(\"Crop Yield Graphs of Different Countries\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e8103c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql' has no attribute 'sql'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#graph of Top 20 highest average rainfall area wise \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43msql\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mSELECT Area, average_rain_fall_mm_per_year from crop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m df = df.toPandas()\n\u001b[32m      4\u001b[39m df = df.groupby(\u001b[33m'\u001b[39m\u001b[33mArea\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33maverage_rain_fall_mm_per_year\u001b[39m\u001b[33m'\u001b[39m].mean().sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).nlargest(\u001b[32m20\u001b[39m).reset_index()\n",
      "\u001b[31mAttributeError\u001b[39m: module 'pyspark.sql' has no attribute 'sql'"
     ]
    }
   ],
   "source": [
    "#graph of Top 20 highest average rainfall area wise \n",
    "df = sql.sql(\"SELECT Area, average_rain_fall_mm_per_year from crop\")\n",
    "df = df.toPandas()\n",
    "df = df.groupby('Area')['average_rain_fall_mm_per_year'].mean().sort_values(ascending=False).nlargest(20).reset_index()\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(df['Area'], df['average_rain_fall_mm_per_year'])\n",
    "plt.title(\"Top 20 Area Wise Average Rainfall Graph\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78dccfc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql' has no attribute 'sql'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#graph of Top 20 highest area wise pesticides consumption \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43msql\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mSELECT Area, pesticides_tonnes from crop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m df = df.toPandas()\n\u001b[32m      4\u001b[39m df = df.groupby(\u001b[33m'\u001b[39m\u001b[33mArea\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33mpesticides_tonnes\u001b[39m\u001b[33m'\u001b[39m].mean().sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).nlargest(\u001b[32m20\u001b[39m).reset_index()\n",
      "\u001b[31mAttributeError\u001b[39m: module 'pyspark.sql' has no attribute 'sql'"
     ]
    }
   ],
   "source": [
    "#graph of Top 20 highest area wise pesticides consumption \n",
    "df = sql.sql(\"SELECT Area, pesticides_tonnes from crop\")\n",
    "df = df.toPandas()\n",
    "df = df.groupby('Area')['pesticides_tonnes'].mean().sort_values(ascending=False).nlargest(20).reset_index()\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(df['Area'], df['pesticides_tonnes'])\n",
    "plt.title(\"Top 20 Area Wise Average Pesticides Consumption Graph\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7a167e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql' has no attribute 'sql'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringIndexer\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#extracing data for selected country and crop and in above line we are getting all maize yield from India\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df = \u001b[43msql\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mSELECT * from crop where Area=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIndia\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and Item=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMaize\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;66;03m#==============\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#converting Area and Item column from string to numeric vector\u001b[39;00m\n\u001b[32m     10\u001b[39m indexer = StringIndexer(inputCol=\u001b[33m\"\u001b[39m\u001b[33mArea\u001b[39m\u001b[33m\"\u001b[39m, outputCol=\u001b[33m\"\u001b[39m\u001b[33mAreaEncode\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'pyspark.sql' has no attribute 'sql'"
     ]
    }
   ],
   "source": [
    "#split dataset into train and test where application using 80% dataset for training and 20% for testing\n",
    "#extracting train and test features from dataset\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "#extracing data for selected country and crop and in above line we are getting all maize yield from India\n",
    "df = sql.sql(\"SELECT * from crop where Area='India' and Item='Maize'\")#==============\n",
    "#converting Area and Item column from string to numeric vector\n",
    "indexer = StringIndexer(inputCol=\"Area\", outputCol=\"AreaEncode\")\n",
    "encoder = indexer.fit(df)\n",
    "df = encoder.transform(df)\n",
    "#converting Area and Item column from string to numeric vector\n",
    "indexer = StringIndexer(inputCol=\"Item\", outputCol=\"ItemEncode\")\n",
    "encoder = indexer.fit(df)\n",
    "df = encoder.transform(df)\n",
    "#giving required features for training to select\n",
    "requiredColumns = ['Year', 'yield', 'average_rain_fall_mm_per_year', 'pesticides_tonnes','avg_temp', 'AreaEncode', 'ItemEncode']\n",
    "vec_assembler = VectorAssembler(inputCols=requiredColumns, outputCol='train',handleInvalid=\"skip\")\n",
    "transformed = vec_assembler.transform(df)\n",
    "indexer = StringIndexer(inputCol=\"yield\",outputCol=\"predict\",handleInvalid=\"skip\")\n",
    "transformed = indexer.fit(transformed).transform(transformed)\n",
    "#normalizing extracted crop features\n",
    "scaler = MinMaxScaler(inputCol=\"train\", outputCol=\"scaled_train\")\n",
    "transformed = scaler.fit(transformed).transform(transformed)\n",
    "#splitting dataset into train and test\n",
    "(X_train, X_test) = transformed.randomSplit([0.8, 0.2])\n",
    "print(\"80% dataset for training : \"+str(X_train.count()))\n",
    "print(\"20% dataset for testing  : \"+str(X_test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f83381a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RegressionEvaluator\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#creating object of decision tree algorithm\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m dt = \u001b[43mDecisionTreeRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscaled_train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpredict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#training DT on selected train data\u001b[39;00m\n\u001b[32m      6\u001b[39m dt_model = dt.fit(X_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pyspark/__init__.py:139\u001b[39m, in \u001b[36mkeyword_only.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m forces keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m % func.\u001b[34m__name__\u001b[39m)\n\u001b[32m    138\u001b[39m \u001b[38;5;28mself\u001b[39m._input_kwargs = kwargs\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pyspark/ml/regression.py:1142\u001b[39m, in \u001b[36mDecisionTreeRegressor.__init__\u001b[39m\u001b[34m(self, featuresCol, labelCol, predictionCol, maxDepth, maxBins, minInstancesPerNode, minInfoGain, maxMemoryInMB, cacheNodeIds, checkpointInterval, impurity, seed, varianceCol, weightCol, leafCol, minWeightFractionPerNode)\u001b[39m\n\u001b[32m   1134\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1135\u001b[39m \u001b[33;03m__init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[33;03m         maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, \\\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1139\u001b[39m \u001b[33;03m         leafCol=\"\", minWeightFractionPerNode=0.0)\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28msuper\u001b[39m(DecisionTreeRegressor, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m \u001b[38;5;28mself\u001b[39m._java_obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.spark.ml.regression.DecisionTreeRegressor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muid\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1145\u001b[39m kwargs = \u001b[38;5;28mself\u001b[39m._input_kwargs\n\u001b[32m   1146\u001b[39m \u001b[38;5;28mself\u001b[39m.setParams(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pyspark/ml/wrapper.py:80\u001b[39m, in \u001b[36mJavaWrapper._new_java_obj\u001b[39m\u001b[34m(java_class, *args)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[33;03mReturns a new Java object.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     79\u001b[39m sc = SparkContext._active_spark_context\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     82\u001b[39m java_obj = _jvm()\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "#creating object of decision tree algorithm\n",
    "dt = DecisionTreeRegressor(featuresCol=\"scaled_train\", labelCol = 'predict')\n",
    "#training DT on selected train data\n",
    "dt_model = dt.fit(X_train)\n",
    "#perform prediction on test data\n",
    "predict = dt_model.transform(X_test)\n",
    "#collect original crop yield\n",
    "true = predict.select(['predict']).collect()\n",
    "#collect predicted crop yield\n",
    "pred = predict.select(['prediction']).collect()\n",
    "#calculate decision tree performance using RMSE meric \n",
    "evaluator = RegressionEvaluator(labelCol=\"predict\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_error = evaluator.evaluate(predict)\n",
    "print(\"Decision Tree RMSE = \"+str(rmse_error)+\"\\n\")\n",
    "#plot graph of true and predicted crop yield\n",
    "trueYield = []\n",
    "predictedYield = []\n",
    "for i in range(0, 100): \n",
    "    trueYield.append(true[i].predict*100)\n",
    "for i in range(0, 100): \n",
    "    predictedYield.append(pred[i].prediction*100)\n",
    "for i in range(0, 20):\n",
    "    print(\"True Yield = \"+str(trueYield[i])+\" Decision Tree Predicted Yield = \"+str(predictedYield[i]))\n",
    "plt.plot(trueYield, color = 'red', label = 'Original Crop Yield')\n",
    "plt.plot(predictedYield, color = 'green', label = 'Decision Tree Crop Yield')\n",
    "plt.title('Decision Tree True & Predicted Crop Yield Graph')\n",
    "plt.xlabel('Test Data')\n",
    "plt.ylabel('Crop Yield')\n",
    "plt.legend()\n",
    "plt.show()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72789e5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#training linear regression on train features of crop yield dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m lr = \u001b[43mLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscaled_train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpredict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m lr_model = lr.fit(X_train)\n\u001b[32m      5\u001b[39m predict = lr_model.transform(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pyspark/__init__.py:139\u001b[39m, in \u001b[36mkeyword_only.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m forces keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m % func.\u001b[34m__name__\u001b[39m)\n\u001b[32m    138\u001b[39m \u001b[38;5;28mself\u001b[39m._input_kwargs = kwargs\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pyspark/ml/regression.py:328\u001b[39m, in \u001b[36mLinearRegression.__init__\u001b[39m\u001b[34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, standardization, solver, weightCol, aggregationDepth, loss, epsilon, maxBlockSizeInMB)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[33;03m__init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[33;03m         maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \\\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[33;03m         standardization=True, solver=\"auto\", weightCol=None, aggregationDepth=2, \\\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m         loss=\"squaredError\", epsilon=1.35, maxBlockSizeInMB=0.0)\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28msuper\u001b[39m(LinearRegression, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28mself\u001b[39m._java_obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.spark.ml.regression.LinearRegression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muid\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m kwargs = \u001b[38;5;28mself\u001b[39m._input_kwargs\n\u001b[32m    332\u001b[39m \u001b[38;5;28mself\u001b[39m.setParams(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pyspark/ml/wrapper.py:80\u001b[39m, in \u001b[36mJavaWrapper._new_java_obj\u001b[39m\u001b[34m(java_class, *args)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[33;03mReturns a new Java object.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     79\u001b[39m sc = SparkContext._active_spark_context\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     82\u001b[39m java_obj = _jvm()\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "#training linear regression on train features of crop yield dataset\n",
    "lr = LinearRegression(featuresCol=\"scaled_train\", labelCol = 'predict')\n",
    "lr_model = lr.fit(X_train)\n",
    "predict = lr_model.transform(X_test)\n",
    "#collect original crop yield\n",
    "true = predict.select(['predict']).collect()\n",
    "#collect predicted crop yield\n",
    "pred = predict.select(['prediction']).collect()\n",
    "#calculate linear regression performance using RMSE meric \n",
    "evaluator = RegressionEvaluator(labelCol=\"predict\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "lr_rmse_error = evaluator.evaluate(predict)\n",
    "print(\"Linear Regression RMSE = \"+str(lr_rmse_error)+\"\\n\")\n",
    "#plot graph of true and predicted crop yield\n",
    "trueYield = []\n",
    "predictedYield = []\n",
    "for i in range(0, 100): \n",
    "    trueYield.append(true[i].predict*100)\n",
    "for i in range(0, 100): \n",
    "    predictedYield.append(pred[i].prediction*100)\n",
    "for i in range(0, 20):\n",
    "    print(\"True Yield = \"+str(trueYield[i])+\" Linear Regression Predicted Yield = \"+str(predictedYield[i]))\n",
    "plt.plot(trueYield, color = 'red', label = 'Original Crop Yield')\n",
    "plt.plot(predictedYield, color = 'green', label = 'Linear Regression Crop Yield')\n",
    "plt.title('Linear Regression True & Predicted Crop Yield Graph')\n",
    "plt.xlabel('Test Data')\n",
    "plt.ylabel('Crop Yield')\n",
    "plt.legend()\n",
    "plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7b10f50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rmse_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#RMSE comaprison Graph\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m height = [\u001b[43mrmse_error\u001b[49m, lr_rmse_error]\n\u001b[32m      3\u001b[39m bars = [\u001b[33m'\u001b[39m\u001b[33mDecision Tree TMSE\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLinear Regression RMSE\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m y_pos = np.arange(\u001b[38;5;28mlen\u001b[39m(bars))\n",
      "\u001b[31mNameError\u001b[39m: name 'rmse_error' is not defined"
     ]
    }
   ],
   "source": [
    "#RMSE comaprison Graph\n",
    "height = [rmse_error, lr_rmse_error]\n",
    "bars = ['Decision Tree TMSE', 'Linear Regression RMSE']\n",
    "y_pos = np.arange(len(bars))\n",
    "plt.figure(figsize = (4, 3)) \n",
    "plt.bar(y_pos, height)\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.xlabel(\"Algorithm Names\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"RMSE Comparison Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380d0a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27813424-749e-43f5-ae3f-c15855983670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f45e54-1f7c-443c-b433-92c1044d5d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7038f6a-407a-4633-86b3-9bc647bae865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0cd134-a639-4b67-83f2-72f7e0713eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17165ed5-3c81-4083-a87f-8424403ed09e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62aeeec-e1b8-4f03-bda7-07feaf403321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6fb9db-9345-4152-babe-a3a757ccce8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
